# Week 13
df$M.13
# Sum of cases by week
colSums(df)
# Sum of cases by states
rowSums(df, na.rm = TRUE)
# Store all the row sums in variable States
States <- rowSums(df, na.rm = TRUE)
# What is the maximum cases in 2010?
max(States)
# Which states have the maximum cases
names(which.max(States))
# Get the Cases in Perak
df["PERAK",]
# Highest number of cases in Perak
max(df["PERAK",],na.rm = TRUE)
# Which week was that?
names(which.max(df["PERAK",]))
# summary statistics of Week 1 to Week 52
summary(df,na.rm = TRUE)
# summary statistics by states
summary(t(df), na.rm = TRUE)
# What is the maximum cases in 2010?
max(States)
# Which states have the maximum cases
names(which.max(States))
# Get the Cases in Perak
df["PERAK",]
# Highest number of cases in Perak
max(df["PERAK",],na.rm = TRUE)
# Which week was that?
names(which.max(df["PERAK",]))
# summary statistics of Week 1 to Week 52
summary(df,na.rm = TRUE)
# summary statistics by states
summary(t(df), na.rm = TRUE)
source('C:/Users/R/Datasets - MDeC/Example2.R')
source('C:/Users/R/Datasets - MDeC/Example2.R')
source('C:/Users/R/Datasets - MDeC/Example3.R')
# Example 4 - The number of Cases in Melaka from 2010 to 2015 (monthly and yearly)
source('C:/Users/R/Datasets - MDeC/Example4.R')
source('C:/Users/R/Datasets - MDeC/Example4.R')
source('C:/Users/R/Datasets - MDeC/Example5.R')
source('C:/Users/R/Datasets - MDeC/Example5.R')
accuracy(dengue.mdl)
#2 Create column Year based on the generated dates
new_df$Year <- format(as.Date(cut(new_df$daily, breaks = "year")), "%Y")
#2 Create column Year based on the generated dates
new_df$Year <- format(as.Date(cut(new_df$daily, breaks = "year")), "%Y")
source('~/.active-rstudio-document')
#2 Create column Year based on the generated dates
new_df$Year <- format(as.Date(cut(new_df$daily, breaks = "year")), "%Y")
install.packages("C:/Users/R/Datasets - MDeC/R Packages/shiny_0.11.1.zip", repos = NULL)
shiny::runApp('C:/Users/R/Datasets - MDeC/Example 6')
shiny::runApp('C:/Users/R/Datasets - MDeC/Example 6')
library(KernSmooth)
library(datasets)
data(iris)
?iris
iris
mean(iris$Sepal.L)
mean(iris$Sepal.L|iris$Species=='virginica')
apply(iris[, 1:4], 2, mean)
apply(iris, 1, mean)
rowMeans(iris[, 1:4])
apply(iris, 2, mean)
library(datasets)
data(mtcars)
mtcars
sapply(split(mtcars$mpg, mtcars$cyl), mean)
apply(mtcars, 2, mean)
mean(mtcars$mpg, mtcars$cyl)
sapply(mtcars, cyl, mean)
abs(mean(mtcars[mtcarscyl==4,]hp) - mean(mtcars[mtcarscyl==8,]hp))
x = rbind(c(1, -1/4), c(-1/4, 1))
source('C:/Users/XKL0367/ProgrammingAssignment2/cachematrix.R')
m = makeCacheMatrix(x)
m$get()
cacheSolve(m)
cacheSolve(m)
x
swirl()
librar(swirl)
library(swirl)
swirl()
TRUE
TRUE == TRUE
FALSE == TRUE
(FALSE == TRUE) == FALSE
6 == 7
6 < 7
10 <= 10
5 != 7
5 NOT= 7
HELP
help
5 !== 7
!5 == 7
FALSE & FALSe
FALSE & FALSe
FALSE & FALSE
TRUE & c(TRUE, FALSE, FALSE)
TRUE && c(TRUE, FALSE, FALSE)
TRUE | c(TRUE, FALSE, FALSE)
TRUE || c(TRUE, FALSE, FALSE)
5 > 8 || 6 != 8 && 4 > 3.9
isTRUE(6 > 4)
identical('twins', 'twins')
xor(5 == 6, !FALSE)
ints <- sample(10)
ints
ints > 5
which(>7)
which(ints>7)
any(ints<0)
all(ints>0)
Sys.Date()
mean(c(2,4,5))
submit()
boring_function('My
| first function!')
boring_function('My first function!')
boring_function
submit()
my_mean(c(4,5,10))
submit()
help
?
help
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
remainder(5)
remainder(11,5)
remainder(divisor = 11, num = 5)
remainder(4, div = 2)
args(remainder)
submit()
evaluate(c(1.4, 3.6, 7.9, 8.8))
evaluate(std, c(1.4, 3.6, 7.9, 8.8))
evaluate(sd, c(1.4, 3.6, 7.9, 8.8))
evaluate(function(x){x+1}, 6)
evaluate(function(x){x+1}, 6)
help
evaluate(function(x){x[1]}, c(8, 4, 0))
evaluate(function(x){x[-1]}, c(8, 4, 0))
?paste
paste("Programming", "is", "fun!")
submit()
telegram("a","b","c")
submit()
submit()
mad_libs(place = "London", adjustive = "beautiful", noun = "dog")
submit()
submit()
"I" %p% "love" %p% "R!"
set.seed(1)
rpois(5, 2)
set.seed(1)
rpois(5, 2)
pwd()
getwd()
setwd("C:/Users/XKL0367/R")
setwd("C:/Users/XKL0367/R")
setwd("C:/Users/XKL0367/R")
setwd("C:\Users/XKL0367/R")
setwd("C:/Users\XKL0367\R")
setwd("C:/Users/XKL0367/R")
getwd()
setwd("C:\")
setwd("C:\")
setwd("C:\Users")
setwd("C:\Users\XKL0367")
setwd("C:\Users\XKL0367\R")
getwd()
setwd("C:/Users/XKL0367/R")
setwd("C:/Users/XKL0367/R")
setwd("C:/Users/R")
getwd()
outcome <- read.csv("outcome-of-care-measures.csv", colClasses = "character")
head(outcome)
outcome[, 11] <- as.numeric(outcome[, 11])
hist(outcome[, 11])
View(outcome)
View(outcome)
data <- read.csv("outcome-of-care-measures.csv", colClasses = "character")[,c(2,7,11,17,23)]
View(data)
View(data)
source('C:/Users/R/best.R')
best("TX","heart attack")
source('C:/Users/R/best.R')
best("TX","heart attack")
best("TX", "heart failure")
best("MD", "heart attack")
best("BB", "heart attack")
best("NY", "hert attack")
source("submitscript3.R")
submit()
2
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
submit()
source("swirl")
library(swirl)
swirl()
head(flags)
dim(flags)
class(flags)
cls_list <- lapply(flags, class)
cls_list
class(cls_list)
as.character(cls_list)
?sapply
sapply()
sapply(cls_list)
cls_list <- sapply(flags,class)
cls_vect <- sapply(flags, class)
class(cls_vect)
sum(flags$orange)
flag_colors <- flags[,11:17]
head(flag_colors)
lapply(flag_colors,sum)
sapply(flag_colors,sum)
sapply(flag_colors,mean)
flag_shapes <- flags[,19:23]
lapply(flag_shapes,range)
sapply(flag_shapes,shape_mat)
shape_map <- sapply(flag_shapes,range)
shape_mat <- sapply(flag_shapes,range)
shape_mat
class(shape_mat)
unique(3,4,5,5,5,6,6)
unique(c(3, 4, 5, 5, 5, 6, 6))
unique_vals <- lapply(flags,unique)
unique_vals
length(unique_vals)
sapply(unique_vals,length)
sapply(flags,unique)
lapply(unique_vals, function(elem) elem[2])
saaply(flags,unique)
sapply(flags,unique)
ok()
sapply(flags,class)
vapply(flags, class, character(1))
?tapply
table(flags@landmass)
table(flags$landmass)
table(flags$animate)
tapply(flags$animate, flags$landmass, mean)
tapply(flags$population,
| flags$red, summary)
tapply(flags$population,flags$red, summary)
tapply(flags$population,flags$red, summary)
tapply(flags$population,flags$landmass, summary)
ls(0)
ls()
class(plants)
dim(plants)
nrow(plants)
ncol(plants)
object.size(plants)
names(plants)
head(plants)
head(plants)
head(plants,10)
tail(plants,15)
summary(plants)
table(plants$Active_Growth_Period)
str()
str(plants)
?sample
sample(1:6,4,replace= TRUE)
sample(1:6,4,replace= TRUE)
sample(1:20,10)
LETTERS
sample(LETTERS)
prob=c(0.3,0.7)
flips <- prob=c(0.3,0.7)
flips <- sample(c(0,1), 100, replace = TRUE, prob = c(0.3, 0.7))
flips
sum(flips,1)
sum(flips)
?rbinom
rbinom(1,size=1--,prob=0.7)
rbinom(1,size=100,prob=0.7)
flip2 <- rbinom(1,size=100,prob=0.7)
flip2 <- rbinom(100,size=1,prob=0.7)
flips2 <- rbinom(1,size=100,prob=0.7)
flips2 <- rbinom(100,size=1,prob=0.7)
flips2
sum(flips2)
?rnorm
rnorm(10)
rnorm(100,25)
rnorm(10,100,25)
rpois(5,10)
my_pois <- replicate(100,rpois(5,10))
my_pois
cm <- colMeans(my_pois)
hist(cm)
d1 <- Sys.Date()
class(d1)
unclass(d1)
d1
d2 <- as.Date("1969-01-01")
unclass(d2)
t1 <- Sys.time()
t1
class(t1)
unclass(t1)
t2 <- as.POSIXlt(Sys.time())
class(t2)
t2
unclass(t2)
str(unclass(t2))
t2$min
weekdays(d1)
months(t1)
quarters(t2)
t3 <- "October 17, 1986 08:24"
t4 <- strptime(t3, "%B %d, %Y %H:%M")
t4
class(t4)
Sys.time() > t1
Sys.time() - t1
difftime(Sys.time(),t1,units='days')
data(cars)
?cars
head(cars)
plot(cars)
?plot
plot(cars$speed, cars$dist)
plot(cars$dist,cars@speed)
plot(cars$speed, cars$dist)
plot(x=cars$dist,y=cars#speed)
plot(x=cars$dist,y=cars#speed)
plot(x=cars$dist,y=cars#speed)
plot(x=cars$dist,y=cars$speed)
plot(Spped=cars$dist,y=cars$speed)
plot(Speed=cars$dist,y=cars$speed)
plot(x=cars$dist,y=cars$speed,Speed)
plot(x=cars$dist,y=cars$speed,'Speed')
plot(x=cars$dist,y=cars$speed,xlab='Speed')
plot(x=cars$speed,y=cars$dist,xlab='Speed')
plot(x=cars$speed,y=cars$dist,xlab='Speed',ylab="Stopping Distance")
plot(x=cars$speed,y=cars$dist,ylab="Stopping Distance")
plot(x=cars$speed,y=cars$dist,xlab='Speed',ylab="Stopping Distance")
plot(x=cars$speed,y=cars$dist,xlab='Speed',ylab="Stopping Distance")
plot(cars, main="My Plot")
plot(cars, main="My Plot",subtitle="My Plot Subtitle")
plot(cars, sub="My Plot Subtitle")
plot(cars, col=2)
plot(cars, xlim=c(10,15))
plot(cars, pch=2)
mtcars
data(mtcars)
?boxplot
boxplot(formula = mpg~cyl, data=mtcars)
hist(mtcars$mpg)
testData <- read.table("./UCI HAR Dataset/test/X_test.txt",header=FALSE)
testData_sub <- read.table("./UCI HAR Dataset/test/subject_test.txt",header=FALSE)
View(testData_sub)
source('C:/Users/R/run_analysis.R')
# Create output as CSV
source('C:/Users/R/run_analysis.R')
source('C:/Users/R/run_analysis.R')
library("utils", lib.loc="C:/Program Files/R/R-3.1.3/library")
library("utils", lib.loc="C:/Program Files/R/R-3.1.3/library")
pwd
wd
pwd()
setwd
dir
dir()
ls()
getwd()
setwd("~")
getwd()
setwd(./)
setwd("./)
setwd("./")
setwd(.)
setwd(.)
setwd(".")
getwd()
setwd("C:/USers/R")
correlMatrix <- cor(training[, -length(training)])
library(corrplot)
library(randomForest)
library(corrplot)
library(corrplot)
install.packages("corrplot")
install.packages("randomForest")
library(corrplot)
# read the csv file for training
library(caret)
library(kernlab)
library(randomForest)
library(corrplot)
# check if a data folder exists; if not then create one
if (!file.exists("data")) {dir.create("data")}
# file URL and destination file
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
destfile1 <- "./data/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile2 <- "./data/pml-testing.csv"
# download the file and note the time
download.file(fileUrl1, destfile = destfile1)
download.file(fileUrl2, destfile = destfile2)
dateDownloaded <- date()
# read the csv file for training
data_training <- read.csv("./data/pml-training.csv", na.strings= c("NA",""," "))
# clean the data by removing columns with NAs etc
data_training_NAs <- apply(data_training, 2, function(x) {sum(is.na(x))})
data_training_clean <- data_training[,which(data_training_NAs == 0)]
# remove identifier columns such as name, timestamps etc
data_training_clean <- data_training_clean[8:length(data_training_clean)]
# split the cleaned testing data into training and cross validation
inTrain <- createDataPartition(y = data_training_clean$classe, p = 0.7, list = FALSE)
training <- data_training_clean[inTrain, ]
crossval <- data_training_clean[-inTrain, ]
# plot a correlation matrix
correlMatrix <- cor(training[, -length(training)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
# fit a model to predict the classe using everything else as a predictor
model <- randomForest(classe ~ ., data = training)
# crossvalidate the model using the remaining 30% of data
predictCrossVal <- predict(model, crossval)
confusionMatrix(crossval$classe, predictCrossVal)
# apply the same treatment to the final testing data
data_test <- read.csv("./data/pml-testing.csv", na.strings= c("NA",""," "))
data_test_NAs <- apply(data_test, 2, function(x) {sum(is.na(x))})
data_test_clean <- data_test[,which(data_test_NAs == 0)]
data_test_clean <- data_test_clean[8:length(data_test_clean)]
# predict the classes of the test set
predictTest <- predict(model, data_test_clean)
install.packages("caret")
install.packages("kernlab")
library(caret)
library(kernlab)
library(randomForest)
library(corrplot)
# check if a data folder exists; if not then create one
if (!file.exists("data")) {dir.create("data")}
# file URL and destination file
fileUrl1 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
destfile1 <- "./data/pml-training.csv"
fileUrl2 <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
destfile2 <- "./data/pml-testing.csv"
# download the file and note the time
download.file(fileUrl1, destfile = destfile1)
download.file(fileUrl2, destfile = destfile2)
dateDownloaded <- date()
# read the csv file for training
data_training <- read.csv("./data/pml-training.csv", na.strings= c("NA",""," "))
# clean the data by removing columns with NAs etc
data_training_NAs <- apply(data_training, 2, function(x) {sum(is.na(x))})
data_training_clean <- data_training[,which(data_training_NAs == 0)]
# remove identifier columns such as name, timestamps etc
data_training_clean <- data_training_clean[8:length(data_training_clean)]
# split the cleaned testing data into training and cross validation
inTrain <- createDataPartition(y = data_training_clean$classe, p = 0.7, list = FALSE)
training <- data_training_clean[inTrain, ]
crossval <- data_training_clean[-inTrain, ]
# plot a correlation matrix
correlMatrix <- cor(training[, -length(training)])
corrplot(correlMatrix, order = "FPC", method = "circle", type = "lower", tl.cex = 0.8,  tl.col = rgb(0, 0, 0))
# fit a model to predict the classe using everything else as a predictor
model <- randomForest(classe ~ ., data = training)
# crossvalidate the model using the remaining 30% of data
predictCrossVal <- predict(model, crossval)
confusionMatrix(crossval$classe, predictCrossVal)
# apply the same treatment to the final testing data
data_test <- read.csv("./data/pml-testing.csv", na.strings= c("NA",""," "))
data_test_NAs <- apply(data_test, 2, function(x) {sum(is.na(x))})
data_test_clean <- data_test[,which(data_test_NAs == 0)]
data_test_clean <- data_test_clean[8:length(data_test_clean)]
# predict the classes of the test set
predictTest <- predict(model, data_test_clean)
cd
cd PredictionAssignmentWriteup
pwd
pwd()
wd()
pwd()
setwd
setwd(C:/Users/R/PredictionAssignmentWriteup)
setwd('C:/Users/R/PredictionAssignmentWriteup')
setwd("C:/Users/R/PredictionAssignmentWriteup")
setwd("C:/Users/R/PredictionAssignmentWriteup")
setwd('C:/Users/R/PredictionAssignmentWriteup')
setwd('C:/Users/R/PredictionAssignmentWriteup')
setwd("C:/Users/R/PredictionAssignmentWriteup")
setwd("C:/Users/R/PredictionAssignmentWriteup")
library("caret", lib.loc="~/R/win-library/3.1")
library("corrplot", lib.loc="~/R/win-library/3.1")
library("kernlab", lib.loc="~/R/win-library/3.1")
library("knitr", lib.loc="~/R/win-library/3.1")
library("randomForest", lib.loc="~/R/win-library/3.1")
